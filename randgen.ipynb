{
 "metadata": {
  "name": "",
  "signature": "sha256:a908382dade1eb13e181972c09a0634dfd92311776501fe337272a984319fc9b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Entropy Sources"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\"Nothing comes from nothing.\""
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "/dev/random"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "+ Device on UNIX-like systems that provides a blocking source of entropy.\n",
      "+ Reading from /dev/random will cause the system to return bits generated by applying PRNG algorithms to inputs from the environment.\n",
      "+ Will block if you read so much that the system runs out of bits to give you! If this happens, move the mouse around to generate more environmental noise for the device.\n",
      "+ /dev/urandom is the non-blocking counterpart, but has fewer randomness guarantees."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "RdRand"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "+ Intel processor instruction to put random bits into a target register.\n",
      "+ Presumably uses on-chip noise (heat, voltage), but implementation is private.\n",
      "+ Concerns about NSA backdoors.\n",
      "+ Used along with other sources of entropy in x86 Linux implementation of `/dev/random`"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "A quick puzzle"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Given as input a bit generator that (independently and at random) generates 1 with probability *p* and 0 with probability *1-p*, return as output a bit generator that independently and at random generates 1 and 0 probability 0.5 each."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Generating Uniform Random Numbers"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "+ Now that we can generate random bits, generating uniform random `int`s is trivial.\n",
      "+ Also we can scale these `int`s by a normalizing constant to get uniform random numbers in any desired range (e.g., (0,1))"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Sampling from Other Distributions"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Rejection Sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO : talk about generalizations of rejection sampling, downsides."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "def rejection_sample(f, num_samples=1):\n",
      "    if num_samples <= 0:\n",
      "        return np.empty(0)\n",
      "    else:\n",
      "        samples = np.empty(num_samples)\n",
      "        samples[:] = np.nan\n",
      "        \n",
      "        for i in range(num_samples):\n",
      "            while np.isnan(samples[i]):\n",
      "                candidate = np.random.random()\n",
      "                if (np.random.random() < f(candidate)):\n",
      "                    samples[i] = candidate\n",
      "            \n",
      "        return samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Inverse transform sampling"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we want to sample from a distribution with an invertible, monotone increasing *cumulative* distribution function\n",
      "$\\Phi$, where we know how to compute $\\Phi^{-1}$ (also an increasing function).\n",
      "\n",
      "Then the following algorithm works:\n",
      "    \n",
      "   1. Sample $u \\sim U([0, 1])$\n",
      "   2. Return $x = \\Phi^{-1}(u)$\n",
      "    \n",
      "Proof of correctness:\n",
      "\n",
      "By definition of the CDF, it suffices to show that, for all $x \\in \\mathbb{R}$, the probability of getting a value less \n",
      "than or equal to $x$ from the above algorithm is $\\Phi(x)$.\n",
      "\n",
      "Consider any $x \\in \\mathbb{R}$.\n",
      "\n",
      "We will get a value less than or equal to $x$ from the above algorithm when and only when the number $u$ that was\n",
      "randomly sampled from the $U([0, 1])$ distribution in step 1 is less than $\\Phi(x)$ (convince yourself of this; it is because of the monotonicity of $\\Phi^{-1}$). But by definition of the uniform distribution, $\\mathbb{P} (u \\leq \\Phi(x)) \\equiv \\Phi(x)$.\n",
      "Q.E.D."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def inverse_transform_sample(Phi_inv, num_samples=1):\n",
      "    if num_samples <= 0:\n",
      "        return np.empty(0)\n",
      "    else:\n",
      "        return np.fromiter((Phi_inv(u) for u in np.random.rand(num_samples)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Markov Chain Monte Carlo (MCMC) Methods"
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Markov chain fundamentals"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Markov chain is the probabilistic cousin of the nondeterministic finite-state automaton (NFA). It can be represented by a directed graph where the nodes are *states* and the arcs are *transitions* between states. Each arc $i \\rightarrow j$ is weighted with a nonnegative real weight that represents the probability of going from state $i$ to state $j$ in a single transition. This transition probability is denoted $P(i \\rightarrow j)$ or $P(j | i)$. Naturally, for each state $i$, the sum of all transition probabilities out of $i$ must be 1. Often times we will work with the transition probabilities by way of a transition probability matrix $P$ with $P_{ij} = P(i \\rightarrow j)$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Metropolis-Hastings Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Suppose we want to sample from a distribution $\\pi$.\n",
      "\n",
      "Idea: construct a Markov chain with transition probability $P(x \\rightarrow y)$ whose stationary distribution is $\\pi$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Regularity"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Loosely speaking, a Markov chain is *regular* if it is possible to get from any state to any other state after a\n",
      "certain period of time. More rigorously, if $P$ is the transition probability matrix of the chain, then there\n",
      "exists some positive integer $n$ such that all entries of $P^n$ are strictly positive. It can be shown that, if\n",
      "this is the case, then for all integers $k = n+1, n+2, n+3, \\ldots$, $P^k$ also has only strictly positive entries."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Detailed Balance"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Together with regularity, having *detailed balance* provides a sufficient condition for a Markov chain to have a unique stationary distribution.\n",
      "\n",
      "Assume a regular Markov chain with transition probability distribution $P(x \\rightarrow y)$. If there exists a distribution $\\pi$\n",
      "such that, for all $x$, $y$, $\\pi(x) P(x \\rightarrow y) = \\pi(y) P(y \\rightarrow x)$, then the chain is said to have detailed balance. Moreover, $\\pi$ is the unique stationary distribution of the chain. A proof is left as an exercise to the reader."
     ]
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "The Algorithm"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now the question is, how do we come up with this magical Markov chain $P$?\n",
      "\n",
      "Let's split up a transition out of state $x$ into two steps: a *proposal* to go to some state $y$, and then an *acceptance* of that proposal with some probability. That is, we will say $P(x \\rightarrow y) = Q(x \\rightarrow y) A(x \\rightarrow y)$. The advantage of doing this is that it affords us some flexibility in constructing the Markov chain: we can have $Q$ be whatever distribution we want, and then solve for $A$. The distribution $Q$ is called the *proposal distribution*, and $A$ is the *acceptance distribution*. In pseudocode, the algorithm is as follows:\n",
      "\n",
      "1. Start out with an initial state $x = x_0$.\n",
      "\n",
      "2. Continuously do the following:\n",
      "\n",
      "    a. Get a proposal for the next state $y$ by sampling from $Q(x \\rightarrow y) = Q(y | x)$\n",
      "    \n",
      "    b. With probability $A(x \\rightarrow y)$, accept the proposal and set $x = y$. Otherwise, $x$ does not change.\n",
      "    \n",
      "Provided that we choose $Q$ and $A$ correctly, the stationary distribution of $x$ will be the desired distribution $\\pi$. Now, of course, the question is *how* do we choose $Q$ and $A$ correctly?\n",
      "\n",
      "$Q$ can be whatever we want. However, a choice of $Q$ (along with a desired target distribution $\\pi$) completely determines $A$.\n",
      "\n",
      "Recall that we need $\\pi(x) P(x \\rightarrow y) = \\pi(y) P(y \\rightarrow x)$, because these are the detailed balance equations. By construction of $Q$ and $A$, we have $P(x \\rightarrow y) = Q(x \\rightarrow y) A(x \\rightarrow y)$. So, the conditions of details balance become $\\pi(x) Q(x \\rightarrow y) A(x \\rightarrow y) = \\pi(y) Q(y \\rightarrow x) A(y \\rightarrow x)$.\n",
      "\n",
      "This implies the following criterion on the acceptance distribution: $$\\frac{A(x \\rightarrow y)}{A(y \\rightarrow x)} = \\frac{\\pi(y) Q(y \\rightarrow x)}{\\pi(x) Q(x \\rightarrow y)}$$\n",
      "\n",
      "To satisfy this criterion, it suffices to set $$A(x \\rightarrow y) = \\min \\left[ 1, \\frac{\\pi(y) Q(y \\rightarrow x)}{\\pi(x) Q(x \\rightarrow y)} \\right]$$ (convince yourself of this)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Q is the proposal distribution.\n",
      "# Q should be a function that, when applied to an argument x: float, returns a\n",
      "# function of a single argument y: float such that Q(x)(y) = Q(x -> y)\n",
      "# (the Metropolis-Hastings proposal distribution for transition).\n",
      "\n",
      "# for example:\n",
      "# def Q(x):\n",
      "#     def Q_x(y):\n",
      "#        return norm_pdf(y, mean=x, sd=1)\n",
      "#     return Q_x\n",
      "\n",
      "def metropolis_hastings(pi, Q, num_samples=1, burn_in=10000):\n",
      "    if num_samples <= 0:\n",
      "        return np.empty(0)\n",
      "    else:\n",
      "        samples = np.empty(num_samples)\n",
      "        \n",
      "        x = np.random.random() # initial state\n",
      "        for i in range(num_samples):\n",
      "            # run Markov chain for burn_in number of steps\n",
      "            for _ in range(burn_in):\n",
      "                proposal_dist = Q(x)\n",
      "                x_proposed = rejection_sample(proposal_dist)\n",
      "                reverse_proposal_dist = Q(x_proposed)\n",
      "                p_accept = min(1, (pi(x_proposed)*reverse_proposal_dist(x))/\n",
      "                                  (pi(x)*proposal_dist(x_proposed)))\n",
      "                \n",
      "                if (np.random.random() < p_accept):\n",
      "                    # move to state x_proposed w.p. p_accept\n",
      "                    x = x_proposed\n",
      "                    \n",
      "            # x is now \"burned in\" and will become the next sample\n",
      "            samples[i] = x\n",
      "            \n",
      "        return samples"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 5,
     "metadata": {},
     "source": [
      "Choosing the Proposal Distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "+ Still not fully understood.\n",
      "+ Opposing forces\n",
      "    + On the one hand, want to assign probability mass to large swathes of the state space to get good mixing and explore the space fully ...\n",
      "    + But if you do this, the acceptance probability is often low, so it takes a long time to transition.\n",
      "+ There are refinements to Metropolis-Hastings which do better\n",
      "    + Multiple-try Metropolis considers multiple proposals at each step, and tries to accept the one with the best chance of success.\n",
      "    + Hybrid (aka Hamiltonian) Monte Carlo models the state space using Hamiltonian dynamics: it considers the space as a differentiable manifold, with \"valleys\" where $\\pi$ is high and \"peaks\" where $\\pi$ is low. Then the state is like a ball rolling through the space, and is drawn towards \"valleys\"."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}